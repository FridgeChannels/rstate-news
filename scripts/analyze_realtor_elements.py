"""
åˆ†æRealtor.comé¡µé¢å…ƒç´ ï¼Œæå–å®é™…é€‰æ‹©å™¨
ä½¿ç”¨ç°æœ‰çš„scraperæ¥è®¿é—®é¡µé¢
"""
import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from scrapers.realtor_scraper import RealtorScraper


async def analyze_realtor_elements():
    """åˆ†æRealtor.comçš„é¡µé¢å…ƒç´ """
    url = "https://www.realtor.com/news/real-estate-news/"
    output_dir = Path(__file__).parent.parent / "analysis" / "realtor"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 70)
    print("Realtor.com é¡µé¢å…ƒç´ åˆ†æ")
    print("=" * 70)
    print(f"URL: {url}\n")
    
    result = {
        'url': url,
        'analyzed_at': datetime.now().isoformat(),
        'article_container_selectors': [],
        'title_selectors': [],
        'link_selectors': [],
        'summary_selectors': [],
        'date_selectors': [],
        'sample_articles': [],
        'recommended_selectors': {}
    }
    
    scraper = RealtorScraper()
    
    try:
        print("ğŸ“ è®¿é—®é¡µé¢...")
        await scraper._setup_browser(headless=True)
        page = await scraper._create_page()
        
        await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        print("  âœ“ é¡µé¢å·²åŠ è½½")
        
        # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
        await asyncio.sleep(5)
        
        # å¤„ç†å¯èƒ½çš„å¼¹çª—
        try:
            close_selectors = [
                "button[aria-label*='close' i]",
                "button[aria-label*='Close' i]",
                ".close-button",
                "[data-testid='close']",
                ".modal-close",
                ".cookie-consent button"
            ]
            for selector in close_selectors:
                try:
                    btn = await page.query_selector(selector)
                    if btn and await btn.is_visible():
                        await btn.click(timeout=2000)
                        await asyncio.sleep(1)
                        print(f"  âœ“ å…³é—­å¼¹çª—: {selector}")
                        break
                except:
                    continue
        except:
            pass
        
        # ç­‰å¾…å†…å®¹åŠ è½½
        await asyncio.sleep(3)
        
        # è·å–HTMLå†…å®¹
        html_content = await page.content()
        
        # ä¿å­˜å®Œæ•´HTML
        html_file = output_dir / "realtor_full_dom.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"  âœ“ HTMLå·²ä¿å­˜: {html_file}")
        
        # ä½¿ç”¨BeautifulSoupåˆ†æDOM
        soup = BeautifulSoup(html_content, 'html.parser')
        
        print("\nğŸ” åˆ†ææ–‡ç« ç»“æ„...")
        
        # å°è¯•æ‰¾åˆ°æ–‡ç« å®¹å™¨
        # æ ¹æ®ç”¨æˆ·æä¾›çš„HTMLï¼Œæ–‡ç« å®¹å™¨å¯èƒ½æ˜¯ div.sc-1ri3r0p-0 æˆ–åŒ…å«Cardstylesçš„å…ƒç´ 
        article_containers = []
        
        # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«ç‰¹å®šç±»åçš„div
        potential_containers = soup.find_all('div', class_=lambda x: x and ('sc-1ri3r0p-0' in str(x) or 'Cardstyles' in str(x)))
        article_containers.extend(potential_containers)
        
        # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«æ–°é—»é“¾æ¥çš„å®¹å™¨
        news_links = soup.find_all('a', href=lambda x: x and '/news/real-estate-news/' in str(x))
        print(f"  æ‰¾åˆ° {len(news_links)} ä¸ªæ–°é—»é“¾æ¥")
        
        for link in news_links[:10]:  # åªå–å‰10ä¸ª
            container = link.find_parent('div', class_=lambda x: x and ('card' in str(x).lower() or 'article' in str(x).lower() or 'Card' in str(x)))
            if container and container not in article_containers:
                article_containers.append(container)
        
        print(f"  æ‰¾åˆ° {len(article_containers)} ä¸ªæ½œåœ¨æ–‡ç« å®¹å™¨")
        
        # åˆ†æå‰5ä¸ªå®¹å™¨çš„ç»“æ„
        for i, container in enumerate(article_containers[:5]):
            print(f"\n  ğŸ“„ åˆ†ææ–‡ç« å®¹å™¨ {i+1}:")
            
            article_info = {
                'container_index': i,
                'container_classes': container.get('class', []),
                'container_id': container.get('id'),
                'title': None,
                'title_selector': None,
                'link': None,
                'link_selector': None,
                'summary': None,
                'summary_selector': None,
                'date': None,
                'date_selector': None
            }
            
            # åˆ†ææ ‡é¢˜
            title_elements = []
            # æŸ¥æ‰¾ h3.sc-1ewhvwh-0
            h3_elements = container.find_all('h3', class_=lambda x: x and 'sc-1ewhvwh-0' in str(x))
            title_elements.extend(h3_elements)
            # æŸ¥æ‰¾ h3[font-weight="bold"]
            h3_bold = container.find_all('h3', attrs={'font-weight': 'bold'})
            title_elements.extend(h3_bold)
            # æŸ¥æ‰¾æ‰€æœ‰h3
            if not title_elements:
                title_elements = container.find_all('h3')
            
            if title_elements:
                title_elem = title_elements[0]
                article_info['title'] = title_elem.get_text(strip=True)
                article_info['title_selector'] = _build_selector(title_elem)
                print(f"    æ ‡é¢˜: {article_info['title'][:60]}...")
                print(f"    æ ‡é¢˜é€‰æ‹©å™¨: {article_info['title_selector']}")
            
            # åˆ†æé“¾æ¥
            link_elements = []
            # æŸ¥æ‰¾åŒ…å« /news/real-estate-news/ çš„é“¾æ¥
            links = container.find_all('a', href=lambda x: x and '/news/real-estate-news/' in str(x))
            link_elements.extend(links)
            # æŸ¥æ‰¾æ ‡é¢˜å†…çš„é“¾æ¥
            if title_elements:
                title_links = title_elements[0].find_all('a')
                link_elements.extend(title_links)
            
            if link_elements:
                link_elem = link_elements[0]
                article_info['link'] = link_elem.get('href', '')
                article_info['link_selector'] = _build_selector(link_elem)
                print(f"    é“¾æ¥: {article_info['link']}")
                print(f"    é“¾æ¥é€‰æ‹©å™¨: {article_info['link_selector']}")
            
            # åˆ†ææ‘˜è¦
            summary_elements = []
            # æŸ¥æ‰¾ p.dsOTPE æˆ–åŒ…å« dsOTPE çš„p
            p_elements = container.find_all('p', class_=lambda x: x and 'dsOTPE' in str(x))
            summary_elements.extend(p_elements)
            # æŸ¥æ‰¾ card-content å†…çš„æ®µè½
            card_content = container.find('div', class_=lambda x: x and 'card-content' in str(x).lower())
            if card_content:
                p_in_content = card_content.find_all('p')
                summary_elements.extend(p_in_content)
            
            if summary_elements:
                summary_elem = summary_elements[0]
                article_info['summary'] = summary_elem.get_text(strip=True)
                article_info['summary_selector'] = _build_selector(summary_elem)
                print(f"    æ‘˜è¦: {article_info['summary'][:60]}...")
                print(f"    æ‘˜è¦é€‰æ‹©å™¨: {article_info['summary_selector']}")
            
            # åˆ†ææ—¥æœŸ
            date_elements = []
            # æŸ¥æ‰¾ time å…ƒç´ 
            time_elements = container.find_all('time')
            date_elements.extend(time_elements)
            # æŸ¥æ‰¾åŒ…å«æ—¥æœŸçš„å…ƒç´ 
            date_patterns = ['date', 'time', 'published', 'publish']
            for pattern in date_patterns:
                date_elems = container.find_all(attrs={'class': lambda x: x and pattern in str(x).lower()})
                date_elements.extend(date_elems)
            
            if date_elements:
                date_elem = date_elements[0]
                article_info['date'] = date_elem.get_text(strip=True) or date_elem.get('datetime', '')
                article_info['date_selector'] = _build_selector(date_elem)
                print(f"    æ—¥æœŸ: {article_info['date']}")
                print(f"    æ—¥æœŸé€‰æ‹©å™¨: {article_info['date_selector']}")
            
            result['sample_articles'].append(article_info)
        
        # æ±‡æ€»æ¨èçš„é€‰æ‹©å™¨
        print("\nğŸ“‹ æ¨èçš„é€‰æ‹©å™¨:")
        
        # æ–‡ç« å®¹å™¨é€‰æ‹©å™¨
        if article_containers:
            container_classes = set()
            for container in article_containers[:5]:
                classes = container.get('class', [])
                container_classes.update(classes)
            
            # æ‰¾å‡ºæœ€å¸¸è§çš„ç±»åæ¨¡å¼
            recommended_container = None
            for cls in container_classes:
                if 'sc-' in cls or 'Card' in cls:
                    recommended_container = f"div.{cls}" if cls else "div[class*='Card']"
                    break
            
            if not recommended_container:
                recommended_container = "div[class*='card']"
            
            result['recommended_selectors']['article_container'] = recommended_container
            print(f"  æ–‡ç« å®¹å™¨: {recommended_container}")
        
        # æ ‡é¢˜é€‰æ‹©å™¨
        title_selectors = [a['title_selector'] for a in result['sample_articles'] if a.get('title_selector')]
        if title_selectors:
            # æ‰¾å‡ºæœ€å¸¸è§çš„
            recommended_title = title_selectors[0]
            result['recommended_selectors']['title'] = recommended_title
            print(f"  æ ‡é¢˜: {recommended_title}")
        
        # é“¾æ¥é€‰æ‹©å™¨
        link_selectors = [a['link_selector'] for a in result['sample_articles'] if a.get('link_selector')]
        if link_selectors:
            recommended_link = "a[href*='/news/real-estate-news/']"  # æ›´é€šç”¨çš„é€‰æ‹©å™¨
            result['recommended_selectors']['link'] = recommended_link
            print(f"  é“¾æ¥: {recommended_link}")
        
        # æ‘˜è¦é€‰æ‹©å™¨
        summary_selectors = [a['summary_selector'] for a in result['sample_articles'] if a.get('summary_selector')]
        if summary_selectors:
            recommended_summary = summary_selectors[0]
            result['recommended_selectors']['summary'] = recommended_summary
            print(f"  æ‘˜è¦: {recommended_summary}")
        
        # æ—¥æœŸé€‰æ‹©å™¨
        date_selectors = [a['date_selector'] for a in result['sample_articles'] if a.get('date_selector')]
        if date_selectors:
            recommended_date = date_selectors[0] if date_selectors else "time"
            result['recommended_selectors']['date'] = recommended_date
            print(f"  æ—¥æœŸ: {recommended_date}")
        
        # ä¿å­˜åˆ†æç»“æœ
        json_file = output_dir / "realtor_analysis.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"\n  âœ“ åˆ†æç»“æœå·²ä¿å­˜: {json_file}")
        
        print("\n" + "=" * 70)
        print("âœ… åˆ†æå®Œæˆï¼")
        print("=" * 70)
        
        return result
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return result
    finally:
        await scraper.cleanup()


def _build_selector(element):
    """æ„å»ºå…ƒç´ çš„é€‰æ‹©å™¨"""
    if not element:
        return None
    
    # ä¼˜å…ˆä½¿ç”¨ID
    if element.get('id'):
        return f"{element.name}#{element.get('id')}"
    
    # ä½¿ç”¨ç±»å
    classes = element.get('class', [])
    if classes:
        class_str = '.'.join(classes)
        return f"{element.name}.{class_str}"
    
    # ä½¿ç”¨å±æ€§
    if element.get('data-testid'):
        return f"{element.name}[data-testid='{element.get('data-testid')}']"
    
    # ä½¿ç”¨éƒ¨åˆ†ç±»ååŒ¹é…ï¼ˆå¯¹äºåŠ¨æ€ç±»åï¼‰
    if classes:
        # æ‰¾å‡ºåŒ…å«ç‰¹æ®Šå­—ç¬¦çš„ç±»åï¼ˆé€šå¸¸æ˜¯åŠ¨æ€ç”Ÿæˆçš„ï¼‰
        for cls in classes:
            if 'sc-' in cls or '__' in cls:
                return f"{element.name}[class*='{cls.split('-')[0] if '-' in cls else cls[:5]}']"
    
    # å›é€€åˆ°æ ‡ç­¾å
    return element.name


if __name__ == "__main__":
    asyncio.run(analyze_realtor_elements())
