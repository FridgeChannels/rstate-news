"""
äº¤äº’å¼Scraperæµ‹è¯•å·¥å…·
é€ä¸ªæµ‹è¯•æ¯ä¸ªscraperï¼ŒéªŒè¯å…ƒç´ æå–
"""
import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from scrapers.newsbreak_scraper import NewsbreakScraper
from scrapers.patch_scraper import PatchScraper
from scrapers.realtor_scraper import RealtorScraper
from scrapers.redfin_scraper import RedfinScraper
from scrapers.nar_scraper import NARScraper
from scrapers.freddiemac_scraper import FreddieMacScraper
from utils.logger import logger


async def test_scraper(scraper_class, name: str, zipcode: str = None):
    """æµ‹è¯•å•ä¸ªscraper"""
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•: {name}")
    print(f"{'='*70}\n")
    
    try:
        scraper = scraper_class()
        
        if zipcode:
            print(f"ğŸ“Œ ä½¿ç”¨Zipcode: {zipcode}")
            articles = await scraper.scrape(zipcode=zipcode, limit=5)
        else:
            print("ğŸ“Œ é‡‡é›†æˆ¿åœ°äº§æ–°é—»")
            articles = await scraper.scrape(limit=5)
        
        print(f"\nâœ… é‡‡é›†ç»“æœ: {len(articles)} ç¯‡æ–‡ç« \n")
        
        for i, article in enumerate(articles[:3], 1):
            print(f"æ–‡ç«  {i}:")
            print(f"  æ ‡é¢˜: {article.get('title', 'N/A')[:60]}...")
            print(f"  é“¾æ¥: {article.get('url', 'N/A')[:60]}...")
            print(f"  æ—¥æœŸ: {article.get('publish_date', 'N/A')}")
            print(f"  å†…å®¹: {article.get('content', article.get('content_summary', 'N/A'))[:60]}...")
            print()
        
        if len(articles) > 0:
            print(f"âœ… {name} æµ‹è¯•æˆåŠŸï¼èƒ½æå–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            return True
        else:
            print(f"âŒ {name} æµ‹è¯•å¤±è´¥ï¼šæœªèƒ½æå–åˆ°æ–‡ç« ")
            return False
            
    except Exception as e:
        print(f"âŒ {name} æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("äº¤äº’å¼Scraperæµ‹è¯•å·¥å…·")
    print("="*70)
    
    test_cases = [
        (NewsbreakScraper, "Newsbreak", "90210"),
        (PatchScraper, "Patch", "90210"),
        (RealtorScraper, "Realtor.com", None),
        (RedfinScraper, "Redfin", None),
        (NARScraper, "NAR", None),
        (FreddieMacScraper, "Freddie Mac", None),
    ]
    
    results = {}
    
    for scraper_class, name, zipcode in test_cases:
        success = await test_scraper(scraper_class, name, zipcode)
        results[name] = success
        await asyncio.sleep(3)  # ç½‘ç«™ä¹‹é—´å»¶è¿Ÿ
    
    print("\n" + "="*70)
    print("æµ‹è¯•æ€»ç»“:")
    print("="*70)
    for name, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
    print("="*70)


if __name__ == "__main__":
    asyncio.run(main())
