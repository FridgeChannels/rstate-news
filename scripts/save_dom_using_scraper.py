"""
ä½¿ç”¨ç°æœ‰çš„scraperç±»è®¿é—®ç½‘ç«™å¹¶ä¿å­˜DOM
è¿™æ ·å¯ä»¥å¤ç”¨å·²æœ‰çš„èµ„æºç®¡ç†é€»è¾‘
"""
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from scrapers.newsbreak_scraper import NewsbreakScraper
from scrapers.patch_scraper import PatchScraper
from scrapers.realtor_scraper import RealtorScraper
from scrapers.redfin_scraper import RedfinScraper
from scrapers.nar_scraper import NARScraper
from scrapers.freddiemac_scraper import FreddieMacScraper
from utils.logger import logger


SCRAPERS = {
    'newsbreak': (NewsbreakScraper, '90210'),
    'patch': (PatchScraper, '90210'),
    'realtor': (RealtorScraper, None),
    'redfin': (RedfinScraper, None),
    'nar': (NARScraper, None),
    'freddiemac': (FreddieMacScraper, None),
}


async def save_dom(scraper_class, zipcode=None, website_key=''):
    """ä½¿ç”¨scraperè®¿é—®ç½‘ç«™å¹¶ä¿å­˜DOM"""
    output_dir = Path(__file__).parent.parent / "analysis" / "dom_structures"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    scraper = scraper_class()
    
    try:
        await scraper._setup_browser(headless=True)
        page = await scraper._create_page()
        
        # æ ¹æ®scraperç±»å‹ç¡®å®šURL
        if zipcode:
            if scraper.source_name == 'Newsbreak':
                url = f"https://www.newsbreak.com/search?q={zipcode}"
            elif scraper.source_name == 'Patch':
                url = f"https://patch.com/search?q={zipcode}"
            else:
                url = scraper.base_url if hasattr(scraper, 'base_url') else ''
        else:
            # å¯¹äºæˆ¿åœ°äº§ç½‘ç«™ï¼Œä½¿ç”¨base_url
            if hasattr(scraper, 'base_url'):
                url = scraper.base_url
            else:
                # æ ¹æ®scraperç±»å‹ç¡®å®šURL
                if scraper.source_name == 'Realtor.com':
                    url = 'https://www.realtor.com/news/real-estate-news/'
                elif scraper.source_name == 'Redfin':
                    url = 'https://www.redfin.com/news/all-redfin-reports/'
                elif scraper.source_name == 'NAR':
                    url = 'https://www.nar.realtor/newsroom'
                elif scraper.source_name == 'Freddie Mac':
                    url = 'https://freddiemac.gcs-web.com/news-releases'
                else:
                    url = ''
        
        if not url:
            print(f"âŒ æ— æ³•ç¡®å®š {scraper.source_name} çš„URL")
            return
        
        print(f"ğŸ“ è®¿é—®: {url}")
        
        # è®¿é—®é¡µé¢
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        except:
            try:
                await page.goto(url, wait_until="load", timeout=60000)
            except:
                await page.goto(url, wait_until="commit", timeout=60000)
        
        await asyncio.sleep(5)  # ç­‰å¾…å†…å®¹åŠ è½½
        
        # è·å–å®Œæ•´DOM
        html_content = await page.content()
        
        # ä¿å­˜HTML
        html_file = output_dir / f"{website_key}_full_dom.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"  âœ“ DOMå·²ä¿å­˜: {html_file}")
        
        # æˆªå›¾
        screenshot_file = output_dir / f"{website_key}_screenshot.png"
        await page.screenshot(path=str(screenshot_file), full_page=True)
        print(f"  âœ“ æˆªå›¾å·²ä¿å­˜: {screenshot_file}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜DOMå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.cleanup()


async def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 scripts/save_dom_using_scraper.py <ç½‘ç«™åç§°>")
        print(f"å¯ç”¨ç½‘ç«™: {', '.join(SCRAPERS.keys())}")
        sys.exit(1)
    
    website_key = sys.argv[1].lower()
    
    if website_key not in SCRAPERS:
        print(f"âŒ æœªçŸ¥ç½‘ç«™: {website_key}")
        print(f"å¯ç”¨ç½‘ç«™: {', '.join(SCRAPERS.keys())}")
        sys.exit(1)
    
    scraper_class, zipcode = SCRAPERS[website_key]
    await save_dom(scraper_class, zipcode, website_key)


if __name__ == "__main__":
    asyncio.run(main())
