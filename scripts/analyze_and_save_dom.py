"""
åˆ†æå¹¶ä¿å­˜æ‰€æœ‰ç½‘ç«™çš„DOMç»“æ„
æ ‡è®°å…³é”®å…ƒç´ ä½ç½®ï¼ˆzipcodeè¾“å…¥æ¡†ã€æ–‡ç« åˆ—è¡¨ç­‰ï¼‰
"""
import asyncio
import json
from pathlib import Path
from datetime import datetime
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup


# ç½‘ç«™é…ç½®
WEBSITES = [
    {
        'name': 'Newsbreak',
        'url': 'https://www.newsbreak.com/search?q=90210',
        'needs_zipcode': True,
        'zipcode_input_selector': None,  # å¾…åˆ†æ
        'article_list_selector': None,  # å¾…åˆ†æ
    },
    {
        'name': 'Patch',
        'url': 'https://patch.com/search?q=90210',
        'needs_zipcode': True,
        'zipcode_input_selector': None,
        'article_list_selector': None,
    },
    {
        'name': 'Realtor.com',
        'url': 'https://www.realtor.com/news/real-estate-news/',
        'needs_zipcode': False,
        'article_list_selector': None,
    },
    {
        'name': 'Redfin',
        'url': 'https://www.redfin.com/news/all-redfin-reports/',
        'needs_zipcode': False,
        'article_list_selector': None,
    },
    {
        'name': 'NAR',
        'url': 'https://www.nar.realtor/newsroom',
        'needs_zipcode': False,
        'article_list_selector': None,
    },
    {
        'name': 'Freddie Mac',
        'url': 'https://freddiemac.gcs-web.com/news-releases',
        'needs_zipcode': False,
        'article_list_selector': None,
    },
]


async def analyze_website(website_info, output_dir: Path):
    """åˆ†æå•ä¸ªç½‘ç«™çš„DOMç»“æ„"""
    name = website_info['name']
    url = website_info['url']
    needs_zipcode = website_info['needs_zipcode']
    
    print(f"\n{'='*70}")
    print(f"åˆ†æ: {name}")
    print(f"URL: {url}")
    print(f"éœ€è¦Zipcode: {needs_zipcode}")
    print(f"{'='*70}\n")
    
    result = {
        'name': name,
        'url': url,
        'needs_zipcode': needs_zipcode,
        'analyzed_at': datetime.now().isoformat(),
        'zipcode_input': None,
        'article_list': None,
        'sample_articles': [],
        'dom_structure': None,
    }
    
    async with async_playwright() as p:
        # ä½¿ç”¨headlessæ¨¡å¼ï¼Œæ›´ç¨³å®š
        browser = await p.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox'
            ]
        )
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        
        try:
            print(f"ğŸ“ è®¿é—®: {url}")
            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            except Exception as e1:
                print(f"  âš ï¸  domcontentloadedè¶…æ—¶ï¼Œå°è¯•load: {str(e1)[:50]}")
                try:
                    await page.goto(url, wait_until="load", timeout=60000)
                except Exception as e2:
                    print(f"  âš ï¸  loadè¶…æ—¶ï¼Œå°è¯•commit: {str(e2)[:50]}")
                    await page.goto(url, wait_until="commit", timeout=60000)
            
            print("  âœ“ é¡µé¢å·²åŠ è½½")
            await asyncio.sleep(5)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            
            # å¤„ç†å¯èƒ½çš„å¼¹çª—
            try:
                close_selectors = [
                    "button[aria-label*='close' i]",
                    "button[aria-label*='Close' i]",
                    ".close-button",
                    "[data-testid='close']",
                    ".modal-close",
                    ".cookie-consent button",
                    "#onetrust-accept-btn-handler"
                ]
                for selector in close_selectors:
                    try:
                        btn = await page.query_selector(selector)
                        if btn and await btn.is_visible():
                            await btn.click(timeout=2000)
                            await asyncio.sleep(1)
                            print(f"  âœ“ å…³é—­å¼¹çª—: {selector}")
                            break
                    except:
                        continue
            except:
                pass
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            await asyncio.sleep(3)
            
            # è·å–å®Œæ•´DOM
            html_content = await page.content()
            result['dom_structure'] = html_content
            
            # ä¿å­˜å®Œæ•´HTML
            html_file = output_dir / f"{name.lower().replace(' ', '_')}_full_dom.html"
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"  âœ“ å®Œæ•´DOMå·²ä¿å­˜: {html_file}")
            
            # ä½¿ç”¨BeautifulSoupè§£æ
            soup = BeautifulSoup(html_content, 'lxml')
            
            # åˆ†æzipcodeè¾“å…¥æ¡†ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if needs_zipcode:
                print("\nğŸ” æŸ¥æ‰¾Zipcodeè¾“å…¥æ¡†...")
                zipcode_selectors = [
                    'input[type="text"][placeholder*="zip" i]',
                    'input[type="text"][placeholder*="zipcode" i]',
                    'input[type="text"][placeholder*="postal" i]',
                    'input[name*="zip" i]',
                    'input[id*="zip" i]',
                    'input[class*="zip" i]',
                    'input[type="search"]',
                ]
                
                for selector in zipcode_selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element and await element.is_visible():
                            # è·å–å…ƒç´ ä¿¡æ¯
                            tag_name = await element.evaluate('el => el.tagName')
                            attributes = await element.evaluate('''el => {
                                const attrs = {};
                                for (let attr of el.attributes) {
                                    attrs[attr.name] = attr.value;
                                }
                                return attrs;
                            }''')
                            bounding_box = await element.bounding_box()
                            
                            result['zipcode_input'] = {
                                'selector': selector,
                                'tag': tag_name.lower(),
                                'attributes': attributes,
                                'position': bounding_box,
                                'xpath': await element.evaluate('''el => {
                                    let path = '';
                                    while (el && el.nodeType === 1) {
                                        let index = 0;
                                        let sibling = el.previousSibling;
                                        while (sibling) {
                                            if (sibling.nodeType === 1 && sibling.tagName === el.tagName) {
                                                index++;
                                            }
                                            sibling = sibling.previousSibling;
                                        }
                                        let tagName = el.tagName.toLowerCase();
                                        let seg = tagName + (index > 0 ? '[' + (index + 1) + ']' : '');
                                        path = '/' + seg + path;
                                        el = el.parentElement;
                                    }
                                    return path;
                                }''')
                            }
                            print(f"  âœ“ æ‰¾åˆ°Zipcodeè¾“å…¥æ¡†: {selector}")
                            print(f"    ä½ç½®: {bounding_box}")
                            break
                    except Exception as e:
                        continue
                
                if not result['zipcode_input']:
                    print("  âš ï¸  æœªæ‰¾åˆ°Zipcodeè¾“å…¥æ¡†ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨åˆ†æ")
            
            # åˆ†ææ–‡ç« åˆ—è¡¨
            print("\nğŸ” æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨...")
            article_selectors = [
                'article',
                '.article-card',
                '.news-item',
                '.story-card',
                '[data-testid*="article" i]',
                '[class*="article" i]',
                '[class*="news" i]',
                '[class*="story" i]',
                '.card',
                'div[class*="item" i]',
            ]
            
            found_articles = []
            for selector in article_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    visible_elements = []
                    for elem in elements[:10]:  # åªæ£€æŸ¥å‰10ä¸ª
                        try:
                            if await elem.is_visible():
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«é“¾æ¥ï¼ˆæ–‡ç« é€šå¸¸æœ‰é“¾æ¥ï¼‰
                                link = await elem.query_selector("a[href]")
                                if link:
                                    visible_elements.append(elem)
                        except:
                            continue
                    
                    if len(visible_elements) >= 3:  # è‡³å°‘æ‰¾åˆ°3ä¸ªå¯è§çš„æ–‡ç« å…ƒç´ 
                        print(f"  âœ“ æ‰¾åˆ° {len(visible_elements)} ä¸ªæ–‡ç« å…ƒç´  (é€‰æ‹©å™¨: {selector})")
                        
                        # åˆ†æç¬¬ä¸€ä¸ªæ–‡ç« å…ƒç´ çš„ç»“æ„
                        first_article = visible_elements[0]
                        
                        # è·å–å…ƒç´ ä¿¡æ¯
                        article_info = {
                            'selector': selector,
                            'count': len(visible_elements),
                            'sample_element': {}
                        }
                        
                        # æŸ¥æ‰¾æ ‡é¢˜
                        title_selectors = ['h1', 'h2', 'h3', 'h4', '.title', '.headline', 'a[href]']
                        for title_sel in title_selectors:
                            try:
                                title_elem = await first_article.query_selector(title_sel)
                                if title_elem:
                                    title_text = await title_elem.inner_text()
                                    if title_text and len(title_text.strip()) > 10:
                                        article_info['sample_element']['title'] = {
                                            'selector': f"{selector} > {title_sel}",
                                            'text': title_text.strip()[:100],
                                        }
                                        break
                            except:
                                continue
                        
                        # æŸ¥æ‰¾é“¾æ¥
                        try:
                            link_elem = await first_article.query_selector("a[href]")
                            if link_elem:
                                href = await link_elem.get_attribute("href")
                                article_info['sample_element']['link'] = {
                                    'selector': f"{selector} > a[href]",
                                    'href': href,
                                }
                        except:
                            pass
                        
                        # æŸ¥æ‰¾æ—¥æœŸ
                        date_selectors = ['time', '.date', '.publish-date', '[datetime]', '.timestamp']
                        for date_sel in date_selectors:
                            try:
                                date_elem = await first_article.query_selector(date_sel)
                                if date_elem:
                                    date_text = await date_elem.inner_text()
                                    datetime_attr = await date_elem.get_attribute("datetime")
                                    if date_text or datetime_attr:
                                        article_info['sample_element']['date'] = {
                                            'selector': f"{selector} > {date_sel}",
                                            'text': date_text,
                                            'datetime': datetime_attr,
                                        }
                                        break
                            except:
                                continue
                        
                        # æŸ¥æ‰¾æ‘˜è¦
                        summary_selectors = ['.summary', '.excerpt', '.description', 'p']
                        for summary_sel in summary_selectors:
                            try:
                                summary_elem = await first_article.query_selector(summary_sel)
                                if summary_elem:
                                    summary_text = await summary_elem.inner_text()
                                    if summary_text and len(summary_text.strip()) > 20:
                                        article_info['sample_element']['summary'] = {
                                            'selector': f"{selector} > {summary_sel}",
                                            'text': summary_text.strip()[:200],
                                        }
                                        break
                            except:
                                continue
                        
                        result['article_list'] = article_info
                        found_articles = visible_elements
                        break
                except Exception as e:
                    continue
            
            if not result['article_list']:
                print("  âš ï¸  æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨åˆ†æ")
            
            # åˆ†æå‰3ä¸ªæ–‡ç« å…ƒç´ çš„è¯¦ç»†ç»“æ„
            if found_articles:
                print(f"\nğŸ“ åˆ†æå‰3ä¸ªæ–‡ç« å…ƒç´ çš„è¯¦ç»†ç»“æ„...")
                for i, article_elem in enumerate(found_articles[:3]):
                    try:
                        article_html = await article_elem.inner_html()
                        article_soup = BeautifulSoup(article_html, 'lxml')
                        
                        article_data = {
                            'index': i + 1,
                            'html_structure': str(article_soup.prettify())[:2000],  # é™åˆ¶é•¿åº¦
                            'text_content': await article_elem.inner_text()[:500],
                        }
                        
                        result['sample_articles'].append(article_data)
                        print(f"  âœ“ æ–‡ç«  {i+1} ç»“æ„å·²åˆ†æ")
                    except Exception as e:
                        print(f"  âš ï¸  åˆ†ææ–‡ç«  {i+1} å¤±è´¥: {str(e)[:50]}")
            
            # æˆªå›¾
            screenshot_file = output_dir / f"{name.lower().replace(' ', '_')}_screenshot.png"
            await page.screenshot(path=str(screenshot_file), full_page=True)
            print(f"\n  âœ“ æˆªå›¾å·²ä¿å­˜: {screenshot_file}")
            
            print(f"\nâœ… {name} åˆ†æå®Œæˆ")
            
        except Exception as e:
            print(f"\nâŒ {name} åˆ†æå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            result['error'] = str(e)
        
        finally:
            await browser.close()
    
    return result


async def main():
    """ä¸»å‡½æ•°"""
    output_dir = Path(__file__).parent.parent / "analysis" / "dom_structures"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*70)
    print("ç½‘ç«™DOMç»“æ„åˆ†æå·¥å…·")
    print("="*70)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"å°†åˆ†æ {len(WEBSITES)} ä¸ªç½‘ç«™\n")
    
    all_results = {}
    
    for website in WEBSITES:
        try:
            result = await analyze_website(website, output_dir)
            all_results[website['name']] = result
            
            # ä¿å­˜å•ä¸ªç½‘ç«™çš„åˆ†æç»“æœ
            json_file = output_dir / f"{website['name'].lower().replace(' ', '_')}_analysis.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f"  âœ“ åˆ†æç»“æœå·²ä¿å­˜: {json_file}\n")
        except Exception as e:
            print(f"  âŒ {website['name']} åˆ†æå¤±è´¥: {str(e)}")
            all_results[website['name']] = {
                'name': website['name'],
                'url': website['url'],
                'error': str(e)
            }
        
        # ç½‘ç«™ä¹‹é—´å»¶è¿Ÿ
        await asyncio.sleep(3)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_file = output_dir / "all_websites_analysis.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆæ ‡è®°æ–‡æ¡£
    generate_markup_document(all_results, output_dir)
    
    print("\n" + "="*70)
    print("âœ… æ‰€æœ‰ç½‘ç«™åˆ†æå®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("="*70)


def generate_markup_document(results, output_dir: Path):
    """ç”Ÿæˆå…ƒç´ æ ‡è®°æ–‡æ¡£"""
    doc_path = output_dir / "ELEMENT_MARKUP.md"
    
    content = """# ç½‘ç«™å…ƒç´ æ ‡è®°æ–‡æ¡£

æœ¬æ–‡æ¡£è®°å½•äº†æ‰€æœ‰ç½‘ç«™çš„å…³é”®å…ƒç´ ä½ç½®ï¼Œç”¨äºæŒ‡å¯¼scraperå¼€å‘ã€‚

ç”Ÿæˆæ—¶é—´: {timestamp}

---

""".format(timestamp=datetime.now().isoformat())
    
    for name, result in results.items():
        content += f"## {name}\n\n"
        content += f"**URL**: {result['url']}\n\n"
        content += f"**éœ€è¦Zipcode**: {'æ˜¯' if result['needs_zipcode'] else 'å¦'}\n\n"
        
        if result.get('error'):
            content += f"**é”™è¯¯**: {result['error']}\n\n"
            content += "---\n\n"
            continue
        
        if result['needs_zipcode'] and result.get('zipcode_input'):
            zipcode = result['zipcode_input']
            content += "### Zipcodeè¾“å…¥æ¡†\n\n"
            content += f"- **é€‰æ‹©å™¨**: `{zipcode['selector']}`\n"
            content += f"- **æ ‡ç­¾**: `{zipcode['tag']}`\n"
            content += f"- **XPath**: `{zipcode.get('xpath', 'N/A')}`\n"
            content += f"- **ä½ç½®**: {zipcode.get('position', {})}\n"
            if zipcode.get('attributes'):
                content += f"- **å±æ€§**:\n"
                for key, value in zipcode['attributes'].items():
                    content += f"  - `{key}`: `{value}`\n"
            content += "\n"
        
        if result.get('article_list'):
            article = result['article_list']
            content += "### æ–‡ç« åˆ—è¡¨\n\n"
            content += f"- **é€‰æ‹©å™¨**: `{article['selector']}`\n"
            content += f"- **æ–‡ç« æ•°é‡**: {article['count']}\n\n"
            
            if article.get('sample_element'):
                content += "#### æ–‡ç« å…ƒç´ ç»“æ„\n\n"
                sample = article['sample_element']
                
                if sample.get('title'):
                    content += f"**æ ‡é¢˜**:\n"
                    content += f"- é€‰æ‹©å™¨: `{sample['title']['selector']}`\n"
                    content += f"- ç¤ºä¾‹æ–‡æœ¬: `{sample['title']['text']}`\n\n"
                
                if sample.get('link'):
                    content += f"**é“¾æ¥**:\n"
                    content += f"- é€‰æ‹©å™¨: `{sample['link']['selector']}`\n"
                    content += f"- ç¤ºä¾‹URL: `{sample['link']['href']}`\n\n"
                
                if sample.get('date'):
                    content += f"**æ—¥æœŸ**:\n"
                    content += f"- é€‰æ‹©å™¨: `{sample['date']['selector']}`\n"
                    if sample['date'].get('text'):
                        content += f"- æ–‡æœ¬: `{sample['date']['text']}`\n"
                    if sample['date'].get('datetime'):
                        content += f"- datetimeå±æ€§: `{sample['date']['datetime']}`\n"
                    content += "\n"
                
                if sample.get('summary'):
                    content += f"**æ‘˜è¦**:\n"
                    content += f"- é€‰æ‹©å™¨: `{sample['summary']['selector']}`\n"
                    content += f"- ç¤ºä¾‹æ–‡æœ¬: `{sample['summary']['text']}`\n\n"
        
        content += "---\n\n"
    
    with open(doc_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"  âœ“ æ ‡è®°æ–‡æ¡£å·²ç”Ÿæˆ: {doc_path}")


if __name__ == "__main__":
    asyncio.run(main())
