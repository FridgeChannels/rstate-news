"""
æµ‹è¯•Issue 004çš„ä¿®å¤
é‡ç‚¹éªŒè¯ï¼š
1. Newsbreakç©ºURLå¤„ç†ï¼ˆdocidä¸ºç©ºæ—¶åº”è¯¥è¿”å›Noneï¼‰
2. Patché€‰æ‹©å™¨è¯­æ³•ä¿®å¤
3. å…¶ä»–æ›´æ–°çš„scraperï¼ˆRedfin, NAR, Freddie Macï¼‰
"""
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from scrapers.newsbreak_scraper import NewsbreakScraper
from scrapers.patch_scraper import PatchScraper
from scrapers.redfin_scraper import RedfinScraper
from scrapers.nar_scraper import NARScraper
from scrapers.freddiemac_scraper import FreddieMacScraper
from utils.logger import logger


async def test_newsbreak():
    """æµ‹è¯•Newsbreak - éªŒè¯JSONæå–å’Œç©ºURLå¤„ç†"""
    print("\n" + "="*70)
    print("æµ‹è¯• Newsbreak Scraper")
    print("="*70)
    
    try:
        scraper = NewsbreakScraper()
        print("ğŸ“Œ å¼€å§‹é‡‡é›† Zipcode: 90210")
        print("â³ è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...\n")
        
        articles = await scraper.scrape(zipcode="90210", limit=5)
        
        print(f"\nâœ… é‡‡é›†å®Œæˆï¼è·å¾— {len(articles)} ç¯‡æ–‡ç« \n")
        
        if articles:
            # éªŒè¯æ‰€æœ‰æ–‡ç« éƒ½æœ‰æœ‰æ•ˆURL
            invalid_urls = [a for a in articles if not a.get('url') or a.get('url') == '']
            if invalid_urls:
                print(f"âŒ å‘ç° {len(invalid_urls)} ç¯‡æ–‡ç« URLä¸ºç©ºï¼ˆä¿®å¤åº”è¯¥å·²è§£å†³æ­¤é—®é¢˜ï¼‰")
                for article in invalid_urls:
                    print(f"  - {article.get('title', 'N/A')[:50]}")
            else:
                print("âœ… æ‰€æœ‰æ–‡ç« éƒ½æœ‰æœ‰æ•ˆURLï¼ˆç©ºURLå¤„ç†ä¿®å¤æˆåŠŸï¼‰")
            
            for i, article in enumerate(articles[:3], 1):
                print(f"\næ–‡ç«  {i}:")
                print(f"  æ ‡é¢˜: {article.get('title', 'N/A')[:60]}...")
                print(f"  é“¾æ¥: {article.get('url', 'N/A')[:60]}...")
                print(f"  æ—¥æœŸ: {article.get('publish_date', 'N/A')}")
                print(f"  å†…å®¹: {article.get('content', article.get('content_summary', ''))[:60]}...")
            
            print("\nâœ… Newsbreak Scraper æµ‹è¯•æˆåŠŸï¼")
            return True
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°æ–‡ç« ï¼ˆå¯èƒ½æ˜¯ç½‘ç«™ç»“æ„å˜åŒ–æˆ–ç½‘ç»œé—®é¢˜ï¼‰")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_patch():
    """æµ‹è¯•Patch - éªŒè¯é€‰æ‹©å™¨ä¿®å¤"""
    print("\n" + "="*70)
    print("æµ‹è¯• Patch Scraper")
    print("="*70)
    
    try:
        scraper = PatchScraper()
        print("ğŸ“Œ å¼€å§‹é‡‡é›† Zipcode: 90210")
        print("â³ è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...\n")
        
        articles = await scraper.scrape(zipcode="90210", limit=5)
        
        print(f"\nâœ… é‡‡é›†å®Œæˆï¼è·å¾— {len(articles)} ç¯‡æ–‡ç« \n")
        
        if articles:
            for i, article in enumerate(articles[:3], 1):
                print(f"æ–‡ç«  {i}:")
                print(f"  æ ‡é¢˜: {article.get('title', 'N/A')[:60]}...")
                print(f"  é“¾æ¥: {article.get('url', 'N/A')[:60]}...")
                print(f"  æ—¥æœŸ: {article.get('publish_date', 'N/A')}")
            
            print("\nâœ… Patch Scraper æµ‹è¯•æˆåŠŸï¼")
            return True
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°æ–‡ç« ï¼ˆå¯èƒ½æ˜¯ç½‘ç«™ç»“æ„å˜åŒ–æˆ–ç½‘ç»œé—®é¢˜ï¼‰")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_redfin():
    """æµ‹è¯•Redfin - éªŒè¯Elementoré€‰æ‹©å™¨"""
    print("\n" + "="*70)
    print("æµ‹è¯• Redfin Scraper")
    print("="*70)
    
    try:
        scraper = RedfinScraper()
        print("ğŸ“Œ å¼€å§‹é‡‡é›†æˆ¿åœ°äº§æ–°é—»")
        print("â³ è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...\n")
        
        articles = await scraper.scrape(limit=5)
        
        print(f"\nâœ… é‡‡é›†å®Œæˆï¼è·å¾— {len(articles)} ç¯‡æ–‡ç« \n")
        
        if articles:
            for i, article in enumerate(articles[:3], 1):
                print(f"æ–‡ç«  {i}:")
                print(f"  æ ‡é¢˜: {article.get('title', 'N/A')[:60]}...")
                print(f"  é“¾æ¥: {article.get('url', 'N/A')[:60]}...")
                print(f"  æ—¥æœŸ: {article.get('publish_date', 'N/A')}")
            
            print("\nâœ… Redfin Scraper æµ‹è¯•æˆåŠŸï¼")
            return True
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°æ–‡ç« ï¼ˆå¯èƒ½æ˜¯ç½‘ç«™ç»“æ„å˜åŒ–æˆ–ç½‘ç»œé—®é¢˜ï¼‰")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_nar():
    """æµ‹è¯•NAR - éªŒè¯Drupal/Next.jsé€‰æ‹©å™¨"""
    print("\n" + "="*70)
    print("æµ‹è¯• NAR Scraper")
    print("="*70)
    
    try:
        scraper = NARScraper()
        print("ğŸ“Œ å¼€å§‹é‡‡é›†æˆ¿åœ°äº§æ–°é—»")
        print("â³ è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...\n")
        
        articles = await scraper.scrape(limit=5)
        
        print(f"\nâœ… é‡‡é›†å®Œæˆï¼è·å¾— {len(articles)} ç¯‡æ–‡ç« \n")
        
        if articles:
            for i, article in enumerate(articles[:3], 1):
                print(f"æ–‡ç«  {i}:")
                print(f"  æ ‡é¢˜: {article.get('title', 'N/A')[:60]}...")
                print(f"  é“¾æ¥: {article.get('url', 'N/A')[:60]}...")
                print(f"  æ—¥æœŸ: {article.get('publish_date', 'N/A')}")
            
            print("\nâœ… NAR Scraper æµ‹è¯•æˆåŠŸï¼")
            return True
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°æ–‡ç« ï¼ˆå¯èƒ½æ˜¯ç½‘ç«™ç»“æ„å˜åŒ–æˆ–ç½‘ç»œé—®é¢˜ï¼‰")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_freddiemac():
    """æµ‹è¯•Freddie Mac - éªŒè¯Drupalé€‰æ‹©å™¨"""
    print("\n" + "="*70)
    print("æµ‹è¯• Freddie Mac Scraper")
    print("="*70)
    
    try:
        scraper = FreddieMacScraper()
        print("ğŸ“Œ å¼€å§‹é‡‡é›†æˆ¿åœ°äº§æ–°é—»")
        print("â³ è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...\n")
        
        articles = await scraper.scrape(limit=5)
        
        print(f"\nâœ… é‡‡é›†å®Œæˆï¼è·å¾— {len(articles)} ç¯‡æ–‡ç« \n")
        
        if articles:
            for i, article in enumerate(articles[:3], 1):
                print(f"æ–‡ç«  {i}:")
                print(f"  æ ‡é¢˜: {article.get('title', 'N/A')[:60]}...")
                print(f"  é“¾æ¥: {article.get('url', 'N/A')[:60]}...")
                print(f"  æ—¥æœŸ: {article.get('publish_date', 'N/A')}")
            
            print("\nâœ… Freddie Mac Scraper æµ‹è¯•æˆåŠŸï¼")
            return True
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°æ–‡ç« ï¼ˆå¯èƒ½æ˜¯ç½‘ç«™ç»“æ„å˜åŒ–æˆ–ç½‘ç»œé—®é¢˜ï¼‰")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("Issue 004 ä¿®å¤éªŒè¯æµ‹è¯•")
    print("="*70)
    print("\næµ‹è¯•å†…å®¹ï¼š")
    print("1. Newsbreak - JSONæå–å’Œç©ºURLå¤„ç†ä¿®å¤")
    print("2. Patch - é€‰æ‹©å™¨è¯­æ³•ä¿®å¤")
    print("3. Redfin - Elementoré€‰æ‹©å™¨æ›´æ–°")
    print("4. NAR - Drupal/Next.jsé€‰æ‹©å™¨æ›´æ–°")
    print("5. Freddie Mac - Drupalé€‰æ‹©å™¨æ›´æ–°")
    
    results = {}
    
    # æµ‹è¯•ä¿®å¤çš„scraper
    print("\n" + "="*70)
    print("æµ‹è¯•ä¿®å¤çš„Scraper")
    print("="*70)
    
    results['Newsbreak'] = await test_newsbreak()
    await asyncio.sleep(3)
    
    results['Patch'] = await test_patch()
    await asyncio.sleep(3)
    
    # æµ‹è¯•æ›´æ–°çš„scraper
    print("\n" + "="*70)
    print("æµ‹è¯•æ›´æ–°çš„Scraper")
    print("="*70)
    
    results['Redfin'] = await test_redfin()
    await asyncio.sleep(3)
    
    results['NAR'] = await test_nar()
    await asyncio.sleep(3)
    
    results['Freddie Mac'] = await test_freddiemac()
    
    # æµ‹è¯•æ€»ç»“
    print("\n" + "="*70)
    print("æµ‹è¯•æ€»ç»“")
    print("="*70)
    for name, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
    
    success_count = sum(1 for v in results.values() if v)
    total_count = len(results)
    print(f"\næ€»è®¡: {success_count}/{total_count} ä¸ªscraperæµ‹è¯•æˆåŠŸ")
    print("="*70)


if __name__ == "__main__":
    asyncio.run(main())
