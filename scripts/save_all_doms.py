"""
æ‰¹é‡ä¿å­˜æ‰€æœ‰ç½‘ç«™çš„DOMç»“æ„
"""
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from scrapers.newsbreak_scraper import NewsbreakScraper
from scrapers.patch_scraper import PatchScraper
from scrapers.realtor_scraper import RealtorScraper
from scrapers.redfin_scraper import RedfinScraper
from scrapers.nar_scraper import NARScraper
from scrapers.freddiemac_scraper import FreddieMacScraper
from utils.logger import logger


SCRAPERS = [
    ('newsbreak', NewsbreakScraper, '90210'),
    ('patch', PatchScraper, '90210'),
    ('realtor', RealtorScraper, None),
    ('redfin', RedfinScraper, None),
    ('nar', NARScraper, None),
    ('freddiemac', FreddieMacScraper, None),
]


async def save_dom(website_key, scraper_class, zipcode=None):
    """ä½¿ç”¨scraperè®¿é—®ç½‘ç«™å¹¶ä¿å­˜DOM"""
    output_dir = Path(__file__).parent.parent / "analysis" / "dom_structures"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    scraper = scraper_class()
    
    try:
        print(f"\n{'='*70}")
        print(f"å¤„ç†: {website_key}")
        print(f"{'='*70}\n")
        
        await scraper._setup_browser(headless=True)
        page = await scraper._create_page()
        
        # æ ¹æ®scraperç±»å‹ç¡®å®šURL
        if zipcode:
            if scraper.source_name == 'Newsbreak':
                url = f"https://www.newsbreak.com/search?q={zipcode}"
            elif scraper.source_name == 'Patch':
                url = f"https://patch.com/search?q={zipcode}"
            else:
                url = scraper.base_url if hasattr(scraper, 'base_url') else ''
        else:
            url = scraper.base_url if hasattr(scraper, 'base_url') else ''
        
        if not url:
            print(f"âŒ æ— æ³•ç¡®å®š {scraper.source_name} çš„URL")
            return
        
        print(f"ğŸ“ è®¿é—®: {url}")
        
        # è®¿é—®é¡µé¢
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        except:
            try:
                await page.goto(url, wait_until="load", timeout=60000)
            except:
                await page.goto(url, wait_until="commit", timeout=60000)
        
        print("  âœ“ é¡µé¢å·²åŠ è½½")
        await asyncio.sleep(5)  # ç­‰å¾…å†…å®¹åŠ è½½
        
        # å¤„ç†å¼¹çª—
        close_selectors = [
            "button[aria-label*='close' i]", ".close-button",
            "[data-testid='close']", ".cookie-consent button"
        ]
        for selector in close_selectors:
            try:
                btn = await page.query_selector(selector)
                if btn and await btn.is_visible():
                    await btn.click(timeout=2000)
                    await asyncio.sleep(1)
                    break
            except:
                continue
        
        await asyncio.sleep(3)
        
        # è·å–å®Œæ•´DOM
        html_content = await page.content()
        
        # ä¿å­˜HTML
        html_file = output_dir / f"{website_key}_full_dom.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"  âœ“ DOMå·²ä¿å­˜: {html_file.name}")
        
        # æˆªå›¾
        screenshot_file = output_dir / f"{website_key}_screenshot.png"
        await page.screenshot(path=str(screenshot_file), full_page=True)
        print(f"  âœ“ æˆªå›¾å·²ä¿å­˜: {screenshot_file.name}")
        
        print(f"  âœ… {website_key} å®Œæˆ")
        
    except Exception as e:
        print(f"  âŒ {website_key} å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.cleanup()


async def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("æ‰¹é‡ä¿å­˜æ‰€æœ‰ç½‘ç«™çš„DOMç»“æ„")
    print("="*70)
    
    output_dir = Path(__file__).parent.parent / "analysis" / "dom_structures"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}\n")
    
    for website_key, scraper_class, zipcode in SCRAPERS:
        try:
            await save_dom(website_key, scraper_class, zipcode)
            await asyncio.sleep(3)  # ç½‘ç«™ä¹‹é—´å»¶è¿Ÿ
        except Exception as e:
            print(f"  âŒ {website_key} å¤„ç†å¤±è´¥: {str(e)}")
            continue
    
    print("\n" + "="*70)
    print("âœ… æ‰€æœ‰ç½‘ç«™å¤„ç†å®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("="*70)


if __name__ == "__main__":
    asyncio.run(main())
