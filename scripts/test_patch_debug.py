"""
Patch Scraperè°ƒè¯•æµ‹è¯•è„šæœ¬
ç”¨äºæ‰‹åŠ¨æµ‹è¯•éªŒè¯Patchç½‘ç«™çš„å·¥ä½œæµç¨‹

ä½¿ç”¨æ–¹æ³•:
    python3 scripts/test_patch_debug.py

æ³¨æ„:
    - è°ƒè¯•æ¨¡å¼ä¼šä»¥å¯è§æ¨¡å¼è¿è¡Œæµè§ˆå™¨ï¼ˆheadless=Falseï¼‰
    - ä¼šåœ¨ logs/patch_debug_screenshots/ ç›®å½•ä¿å­˜æˆªå›¾
    - ä¼šè¾“å‡ºè¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
"""
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from scrapers.patch_scraper import PatchScraper
from utils.logger import logger


async def test_patch_debug():
    """æµ‹è¯•Patch scraperçš„è°ƒè¯•æ¨¡å¼"""
    import sys
    # ç¡®ä¿è¾“å‡ºç«‹å³åˆ·æ–°
    sys.stdout.flush()
    
    print("="*70, flush=True)
    print("Patch Scraper è°ƒè¯•æµ‹è¯•", flush=True)
    print("="*70, flush=True)
    print("\nğŸ“Œ è°ƒè¯•æ¨¡å¼è¯´æ˜:", flush=True)
    print("  - æµè§ˆå™¨å°†ä»¥å¯è§æ¨¡å¼è¿è¡Œï¼ˆheadless=Falseï¼‰", flush=True)
    print("  - æˆªå›¾å°†ä¿å­˜åˆ°: logs/patch_debug_screenshots/", flush=True)
    print("  - è¯·è§‚å¯Ÿæµè§ˆå™¨çª—å£ï¼Œè®°å½•å®é™…è¡Œä¸º", flush=True)
    print("  - æŒ‰Ctrl+Cå¯ä»¥ä¸­æ–­æµ‹è¯•", flush=True)
    print("\n" + "="*70 + "\n", flush=True)
    
    try:
        print("ğŸ” åˆ›å»ºè°ƒè¯•æ¨¡å¼çš„scraper...", flush=True)
        # åˆ›å»ºè°ƒè¯•æ¨¡å¼çš„scraper
        scraper = PatchScraper(debug_mode=True)
        print("âœ… Scraperåˆ›å»ºæˆåŠŸ\n", flush=True)
        
        zipcode = "90210"  # Beverly Hills, CA
        print(f"ğŸ” å¼€å§‹æµ‹è¯•ï¼ŒZipcode: {zipcode}", flush=True)
        print("â³ è¯·è§‚å¯Ÿæµè§ˆå™¨çª—å£...\n", flush=True)
        sys.stdout.flush()
        
        # è¿è¡Œé‡‡é›†ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰
        articles = await scraper.scrape(zipcode=zipcode, limit=5)
        
        print("\n" + "="*70)
        print("æµ‹è¯•å®Œæˆ")
        print("="*70)
        print(f"\nâœ… é‡‡é›†åˆ° {len(articles)} ç¯‡æ–‡ç« \n")
        
        if articles:
            print("æ–‡ç« åˆ—è¡¨:")
            for i, article in enumerate(articles, 1):
                print(f"\næ–‡ç«  {i}:")
                print(f"  æ ‡é¢˜: {article.get('title', 'N/A')[:60]}...")
                print(f"  é“¾æ¥: {article.get('url', 'N/A')[:60]}...")
                print(f"  æ—¥æœŸ: {article.get('publish_date', 'N/A')}")
        else:
            print("âš ï¸ æœªé‡‡é›†åˆ°æ–‡ç« ")
            print("\nè¯·æ£€æŸ¥:")
            print("  1. æµè§ˆå™¨çª—å£ä¸­çš„å®é™…è¡Œä¸º")
            print("  2. logs/patch_debug_screenshots/ ç›®å½•ä¸­çš„æˆªå›¾")
            print("  3. æ§åˆ¶å°è¾“å‡ºçš„è°ƒè¯•æ—¥å¿—")
        
        print("\n" + "="*70)
        print("è°ƒè¯•ä¿¡æ¯:")
        print("="*70)
        print(f"  æˆªå›¾ç›®å½•: logs/patch_debug_screenshots/")
        print(f"  è¯·æŸ¥çœ‹æˆªå›¾å’Œæ—¥å¿—ï¼Œè®°å½•å‘ç°çš„é€‰æ‹©å™¨å’Œè¡Œä¸º")
        print("="*70)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_patch_debug())
