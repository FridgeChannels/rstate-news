"""
åˆ†æå·²ä¿å­˜çš„DOMæ–‡ä»¶ï¼Œæ ‡è®°å…³é”®å…ƒç´ ä½ç½®
"""
import json
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup


WEBSITES = {
    'newsbreak': {
        'name': 'Newsbreak',
        'needs_zipcode': True,
        'html_file': 'newsbreak_full_dom.html',
    },
    'patch': {
        'name': 'Patch',
        'needs_zipcode': True,
        'html_file': 'patch_full_dom.html',
    },
    'realtor': {
        'name': 'Realtor.com',
        'needs_zipcode': False,
        'html_file': 'realtor_full_dom.html',
    },
    'redfin': {
        'name': 'Redfin',
        'needs_zipcode': False,
        'html_file': 'redfin_full_dom.html',
    },
    'nar': {
        'name': 'NAR',
        'needs_zipcode': False,
        'html_file': 'nar_full_dom.html',
    },
    'freddiemac': {
        'name': 'Freddie Mac',
        'needs_zipcode': False,
        'html_file': 'freddiemac_full_dom.html',
    },
}


def analyze_dom_file(website_key, website_info, dom_dir):
    """åˆ†æå•ä¸ªDOMæ–‡ä»¶"""
    name = website_info['name']
    needs_zipcode = website_info['needs_zipcode']
    html_file = dom_dir / website_info['html_file']
    
    if not html_file.exists():
        print(f"  âš ï¸  {name}: DOMæ–‡ä»¶ä¸å­˜åœ¨: {html_file}")
        return None
    
    print(f"\n{'='*70}")
    print(f"åˆ†æ: {name}")
    print(f"{'='*70}\n")
    
    result = {
        'name': name,
        'website_key': website_key,
        'needs_zipcode': needs_zipcode,
        'analyzed_at': datetime.now().isoformat(),
        'zipcode_input': None,
        'article_list': None,
        'sample_articles': [],
    }
    
    try:
        # è¯»å–HTML
        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'lxml')
        
        # åˆ†æzipcodeè¾“å…¥æ¡†
        if needs_zipcode:
            print("ğŸ” æŸ¥æ‰¾Zipcodeè¾“å…¥æ¡†...")
            zipcode_patterns = [
                {'tag': 'input', 'attrs': {'type': 'text', 'placeholder': lambda x: x and 'zip' in x.lower()}},
                {'tag': 'input', 'attrs': {'type': 'text', 'placeholder': lambda x: x and 'postal' in x.lower()}},
                {'tag': 'input', 'attrs': {'name': lambda x: x and 'zip' in x.lower()}},
                {'tag': 'input', 'attrs': {'id': lambda x: x and 'zip' in x.lower()}},
                {'tag': 'input', 'attrs': {'type': 'search'}},
            ]
            
            for pattern in zipcode_patterns:
                elements = soup.find_all(pattern['tag'], pattern['attrs'])
                if elements:
                    elem = elements[0]
                    # æ„å»ºé€‰æ‹©å™¨
                    attrs_list = []
                    for k, v in elem.attrs.items():
                        if k != 'class':
                            if isinstance(v, list):
                                v = ' '.join(v)
                            attrs_list.append(f'{k}="{v}"')
                    selector = f"input[{', '.join(attrs_list)}]" if attrs_list else "input"
                    
                    result['zipcode_input'] = {
                        'tag': elem.name,
                        'attributes': dict(elem.attrs),
                        'selector': selector,
                    }
                    print(f"  âœ“ æ‰¾åˆ°Zipcodeè¾“å…¥æ¡†")
                    print(f"    æ ‡ç­¾: {elem.name}")
                    print(f"    å±æ€§: {elem.attrs}")
                    break
        
        # åˆ†ææ–‡ç« åˆ—è¡¨
        print("\nğŸ” æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨...")
        article_patterns = [
            {'tag': 'article', 'attrs': {}},
            {'tag': 'div', 'attrs': {'class': lambda x: x and 'article' in ' '.join(x).lower()}},
            {'tag': 'div', 'attrs': {'class': lambda x: x and 'news' in ' '.join(x).lower()}},
            {'tag': 'div', 'attrs': {'class': lambda x: x and 'story' in ' '.join(x).lower()}},
            {'tag': 'div', 'attrs': {'class': lambda x: x and 'card' in ' '.join(x).lower()}},
        ]
        
        for pattern in article_patterns:
            attrs = pattern.get('attrs', {})
            if attrs:
                elements = soup.find_all(pattern['tag'], attrs, limit=20)
            else:
                elements = soup.find_all(pattern['tag'], limit=20)
            if len(elements) >= 3:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é“¾æ¥ï¼ˆæ–‡ç« é€šå¸¸æœ‰é“¾æ¥ï¼‰
                articles_with_links = []
                for elem in elements:
                    if elem.find('a', href=True):
                        articles_with_links.append(elem)
                
                if len(articles_with_links) >= 3:
                    print(f"  âœ“ æ‰¾åˆ° {len(articles_with_links)} ä¸ªæ–‡ç« å…ƒç´ ")
                    
                    # åˆ†æç¬¬ä¸€ä¸ªæ–‡ç« 
                    first_article = articles_with_links[0]
                    
                    article_info = {
                        'selector': f"{pattern['tag']}.{first_article.get('class', [])[0] if first_article.get('class') else ''}",
                        'count': len(articles_with_links),
                    }
                    
                    # æŸ¥æ‰¾æ ‡é¢˜
                    title_elem = first_article.find(['h1', 'h2', 'h3', 'h4'])
                    if not title_elem:
                        title_elem = first_article.find('a')
                    if title_elem:
                        article_info['title_selector'] = title_elem.name
                        article_info['title_example'] = title_elem.get_text(strip=True)[:100]
                    
                    # æŸ¥æ‰¾é“¾æ¥
                    link_elem = first_article.find('a', href=True)
                    if link_elem:
                        article_info['link_selector'] = 'a[href]'
                        article_info['link_example'] = link_elem.get('href', '')
                    
                    # æŸ¥æ‰¾æ—¥æœŸ
                    date_elem = first_article.find(['time', 'span'], attrs={'class': lambda x: x and 'date' in ' '.join(x).lower()})
                    if not date_elem:
                        date_elem = first_article.find('time')
                    if date_elem:
                        article_info['date_selector'] = date_elem.name
                        article_info['date_example'] = date_elem.get('datetime') or date_elem.get_text(strip=True)
                    
                    # æŸ¥æ‰¾æ‘˜è¦
                    summary_elem = first_article.find(['p', 'div'], attrs={'class': lambda x: x and any(word in ' '.join(x).lower() for word in ['summary', 'excerpt', 'description'])})
                    if summary_elem:
                        article_info['summary_selector'] = f"{summary_elem.name}.{summary_elem.get('class', [])[0] if summary_elem.get('class') else ''}"
                        article_info['summary_example'] = summary_elem.get_text(strip=True)[:200]
                    
                    result['article_list'] = article_info
                    
                    # ä¿å­˜å‰3ä¸ªæ–‡ç« çš„HTMLç»“æ„
                    for i, article in enumerate(articles_with_links[:3]):
                        result['sample_articles'].append({
                            'index': i + 1,
                            'html': str(article.prettify())[:5000],  # é™åˆ¶é•¿åº¦
                        })
                    
                    break
        
        print(f"\nâœ… {name} åˆ†æå®Œæˆ")
        return result
        
    except Exception as e:
        print(f"âŒ {name} åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def generate_markup_document(all_results, output_dir):
    """ç”Ÿæˆå…ƒç´ æ ‡è®°æ–‡æ¡£"""
    doc_path = output_dir / "ELEMENT_MARKUP.md"
    
    content = f"""# ç½‘ç«™å…ƒç´ æ ‡è®°æ–‡æ¡£

æœ¬æ–‡æ¡£è®°å½•äº†æ‰€æœ‰ç½‘ç«™çš„å…³é”®å…ƒç´ ä½ç½®ï¼Œç”¨äºæŒ‡å¯¼scraperå¼€å‘ã€‚

**ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}

---

"""
    
    for website_key, result in all_results.items():
        if not result:
            continue
        
        content += f"## {result['name']}\n\n"
        content += f"**éœ€è¦Zipcode**: {'æ˜¯' if result['needs_zipcode'] else 'å¦'}\n\n"
        
        if result.get('zipcode_input'):
            zipcode = result['zipcode_input']
            content += "### Zipcodeè¾“å…¥æ¡†\n\n"
            content += f"- **æ ‡ç­¾**: `{zipcode['tag']}`\n"
            content += f"- **é€‰æ‹©å™¨**: `{zipcode.get('selector', 'N/A')}`\n"
            if zipcode.get('attributes'):
                content += f"- **å±æ€§**:\n"
                for key, value in zipcode['attributes'].items():
                    if isinstance(value, list):
                        value = ' '.join(value)
                    content += f"  - `{key}`: `{value}`\n"
            content += "\n"
        
        if result.get('article_list'):
            article = result['article_list']
            content += "### æ–‡ç« åˆ—è¡¨\n\n"
            content += f"- **é€‰æ‹©å™¨**: `{article.get('selector', 'N/A')}`\n"
            content += f"- **æ–‡ç« æ•°é‡**: {article.get('count', 0)}\n\n"
            
            if article.get('title_selector'):
                content += f"**æ ‡é¢˜**:\n"
                content += f"- é€‰æ‹©å™¨: `{article['title_selector']}`\n"
                content += f"- ç¤ºä¾‹: `{article.get('title_example', 'N/A')}`\n\n"
            
            if article.get('link_selector'):
                content += f"**é“¾æ¥**:\n"
                content += f"- é€‰æ‹©å™¨: `{article['link_selector']}`\n"
                content += f"- ç¤ºä¾‹: `{article.get('link_example', 'N/A')[:80]}`\n\n"
            
            if article.get('date_selector'):
                content += f"**æ—¥æœŸ**:\n"
                content += f"- é€‰æ‹©å™¨: `{article['date_selector']}`\n"
                content += f"- ç¤ºä¾‹: `{article.get('date_example', 'N/A')}`\n\n"
            
            if article.get('summary_selector'):
                content += f"**æ‘˜è¦**:\n"
                content += f"- é€‰æ‹©å™¨: `{article['summary_selector']}`\n"
                content += f"- ç¤ºä¾‹: `{article.get('summary_example', 'N/A')[:100]}`\n\n"
        
        content += "---\n\n"
    
    with open(doc_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"\n  âœ“ æ ‡è®°æ–‡æ¡£å·²ç”Ÿæˆ: {doc_path}")


def main():
    """ä¸»å‡½æ•°"""
    dom_dir = Path(__file__).parent.parent / "analysis" / "dom_structures"
    
    print("="*70)
    print("åˆ†æå·²ä¿å­˜çš„DOMæ–‡ä»¶")
    print("="*70)
    print(f"DOMç›®å½•: {dom_dir}\n")
    
    all_results = {}
    
    for website_key, website_info in WEBSITES.items():
        result = analyze_dom_file(website_key, website_info, dom_dir)
        if result:
            all_results[website_key] = result
            
            # ä¿å­˜å•ä¸ªç½‘ç«™çš„åˆ†æç»“æœ
            json_file = dom_dir / f"{website_key}_analysis.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f"  âœ“ åˆ†æç»“æœå·²ä¿å­˜: {json_file.name}\n")
    
    # ç”Ÿæˆæ ‡è®°æ–‡æ¡£
    generate_markup_document(all_results, dom_dir)
    
    print("\n" + "="*70)
    print("âœ… æ‰€æœ‰DOMåˆ†æå®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
