# 去重功能改进实现计划

**Overall Progress:** `100%`

## TLDR

在多个层面实现去重机制：添加URL标准化函数、主流程合并后去重、数据库插入前检查已存在URL，以及可选的数据库唯一约束。解决当前系统大量重复内容的问题。

## Critical Decisions

关键架构/实现决策：
- **URL标准化策略**: 统一URL格式（去除查询参数、fragment、末尾斜杠，http/https统一） - 确保相同内容的不同URL格式能被识别为重复
- **去重层级**: 主流程去重 + 数据库插入前检查（双重保护） - 提高可靠性，即使一层失效也有备份
- **批量查询策略**: 使用IN查询批量检查已存在URL，避免逐条查询 - 提高性能，减少数据库查询次数
- **唯一约束**: 暂不添加数据库唯一约束，先通过应用层去重 - 避免迁移复杂性和插入冲突处理

## Tasks:

- [ ] 🟩 **Step 1: 实现URL标准化函数**
  - [ ] 🟩 在`utils/data_cleaner.py`中添加`normalize_url()`方法
  - [ ] 🟩 统一http/https协议（统一为https）
  - [ ] 🟩 去除URL查询参数（?后的内容）
  - [ ] 🟩 去除URL fragment（#后的内容）
  - [ ] 🟩 去除末尾斜杠（保留根路径的斜杠）
  - [ ] 🟩 处理URL编码（统一大小写）
  - [ ] 🟩 返回标准化后的URL

- [ ] 🟩 **Step 2: 实现主流程去重方法**
  - [ ] 🟩 在`main.py`的`ScraperCoordinator`类中添加`_deduplicate_raw_news()`方法
  - [ ] 🟩 使用URL标准化函数处理每条记录的URL
  - [ ] 🟩 基于标准化URL去重（保留第一次出现的记录）
  - [ ] 🟩 记录去重统计（去重前数量、去重后数量）
  - [ ] 🟩 返回去重后的记录列表

- [ ] 🟩 **Step 3: 在主流程中集成去重**
  - [ ] 🟩 修改`main.py`的`run_scraping_task()`方法
  - [ ] 🟩 在所有scraper采集完成后、插入数据库前调用去重
  - [ ] 🟩 在去重后记录日志（去重前/后数量）
  - [ ] 🟩 确保去重后的数据传递给数据库插入方法

- [ ] 🟩 **Step 4: 实现数据库插入前URL检查**
  - [ ] 🟩 在`database/supabase_client.py`中添加`_check_existing_urls()`方法
  - [ ] 🟩 批量查询已存在的URL（使用IN查询）
  - [ ] 🟩 返回已存在URL的集合
  - [ ] 🟩 处理查询异常（返回空集合，允许继续插入）

- [ ] 🟩 **Step 5: 在插入方法中过滤已存在记录**
  - [ ] 🟩 修改`insert_raw_news()`方法
  - [ ] 🟩 在插入前调用`_check_existing_urls()`检查已存在URL
  - [ ] 🟩 使用URL标准化函数处理待插入记录的URL
  - [ ] 🟩 过滤掉已存在的记录（基于标准化URL）
  - [ ] 🟩 记录跳过数量日志（已存在数量、实际插入数量）
  - [ ] 🟩 只插入新记录

- [ ] 🟩 **Step 6: 添加日志和统计**
  - [ ] 🟩 在主流程去重时记录详细日志
  - [ ] 🟩 在数据库插入时记录跳过数量
  - [ ] 🟩 统计总去重数量（主流程 + 数据库层）
